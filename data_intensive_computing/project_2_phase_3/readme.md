###Counting The Co occurences of authors in various publications.

There are two mappers mapper.py which does the Dijkstra's algorithm and mapperPR.py which does the Pagerank algorithm.
Similarly there are two reducers reducer.py  which does the Dijkstra's algorithm and reducerPR.py which does the Pagerank algorithm.

The original dataset link is https://github.com/bilal-elchami/dijkstra-hadoop-spark/blob/master/data/fb.dat 


To run the MapReduce Program we have to follow the following steps 

1. start-hadoop.sh

    Open a new terminal in the folder that we have the mapper.py, reducer.py and the publications dataset and execute the above command.

2. hdfs dfs -mkdir input

    We need to create a new folder to store the input in the HDFS environment.

3. hdfs dfs -copyFromLocal $HOME myfile.txt

    Then we run the preprocess script preprocess.py to modify the data to remove the weights for all the adjacency list nodes.

4. hdfs dfs -ls 

    We can check if the copy is successful or not using the above command to view the files in that folder.

5. Then we need to run the script using the following command

    python preprocess.py 

    which prints the output to the commad line

6. hadoop jar $HOME/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.6.4.jar \
   -file $HOME/mapper.py -mapper $HOME/mapper.py \
   -file $HOME/reducer.py -reducer $HOME/reducer.py \
   -input input/myfilemodified.txt -output output/Shortestpath

7. hadoop jar $HOME/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.6.4.jar \
   -file $HOME/mapper.py -mapper $HOME/mapperPR.py \
   -file $HOME/reducer.py -reducer $HOME/reducerPR.py \
   -input output/Shortestpath/* -output output/Pagerank

   Then we should execute the above command to run the mapper.py which prints the output to the command line, then the reducer.py which picks the above lines as the input and combines all the input blocks as part of the reducer and outputs the final count of occurances and stores it in the output folder.

6. hdfs dfs -cat output/PageRank/*

    We use this command to view the output generated by the MapReduce program.

7. hdfs dfs -copyToLocal output/ $HOME/output

    Then we copy that output file to the local disk for future use. 

8. stop-hadoop.sh

    Once that is done we can shut down the hadoop cluster

